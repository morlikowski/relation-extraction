# Introduction

Information extraction is a natural language processing task which deals with the identification of real-world entities, such as persons, organizations and places, mentioned in a text and their expressed relations. Information extraction is composed of several subtasks: Entities need to be identified (named entity recognition) in the first place and related to categories (named entity tagging), so that possible relations can be determined (relation extraction). This also includes the need for co-reference resolution, as related entities can be referenced by anaphoric expressions. Depending on the features to be used in these tasks, previous processing might be necessary, e.g. parsing or part-of-speech tagging.

In addition to the described sequentiality, it seems plausible to suggest mutual dependencies between the different subtasks. For example, if we know that a "lives-in" relation holds between two entities A and B, we can be relatively confident that A is a person and B is a place of some kind. Likewise, if we know two entities to be an organization and a place, we can infer that only a subset of all possible relations can hold between them. The same is true for anaphora resolution, for example if we know the anaphora is part of a relation that only applies to persons.

Different approaches vary in the way that they deal with the relations between subtasks. On the one hand of the spectrum, there a pipeline approaches that model the subtasks as a series of consecutive models, each passing its output as input to the sequential model. On the other hand, there are joint inference models, that try to explicitly model the mutual dependencies between the subtasks. In the following, I would like to present an example for each type and detail their principle design and central characteristics (section 2). I then discuss their differences and similarities (section 3) to conclude with a few remarks on potential improvements (section 4).
